{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather necessary pre-requisites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda-latest/envs/neuro/lib/python3.7/site-packages/nilearn/datasets/__init__.py:90: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  \"Numpy arrays.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# import everything you need\n",
    "import logging\n",
    "import h5py  # for interacting with HDF5 binary data format\n",
    "import nipype.pipeline.engine as pe  # pipeline engine\n",
    "import nipype.interfaces.utility as util  # utility\n",
    "from nipype.interfaces.base import Bunch  # Bunch objects fro model specification etc.\n",
    "import nipype.algorithms.modelgen as model  # model generation\n",
    "import nipype.algorithms.rapidart as ra  # artifact detection\n",
    "import nipype.interfaces.fsl as fsl\n",
    "from niflow.nipype1.workflows.fmri.fsl import create_susan_smooth\n",
    "import nipype.interfaces.spm as spm\n",
    "from nipype.utils.filemanip import loadpkl  # to load pklz files (gzipped python storage archives)\n",
    "import nilearn.image as ni_img\n",
    "import nilearn.plotting as ni_plt\n",
    "import nilearn.glm as ni_glm\n",
    "from nilearn import datasets\n",
    "import nltools.mask as nl_mask  # to work with my ROI material\n",
    "import nibabel as nb\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "# activate inline magics\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpl_img\n",
    "import os, operator, re, json, random\n",
    "from functools import reduce\n",
    "from itertools import zip_longest, tee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write update function for file list to data dictionary\n",
    "def collect_files():\n",
    "\n",
    "    '''Collect all current files in data-associated directories and return them in lists by level'''\n",
    "    # collect image files\n",
    "    files = sorted([os.path.join(subdir, content)\n",
    "                   for subdir in FED_dirs\n",
    "                   for content in os.listdir(subdir)\n",
    "                   if re.match(r'.*.(nii|nii.gz|json)$', content)])\n",
    "    # for tissue probability and WM boundaries too (These are the originals! Processed files will be saved in FED_dir^^)\n",
    "    tpms = sorted([os.path.join(subdir, tpm)\n",
    "                  for subdir in TPM_dirs\n",
    "                  for tpm in os.listdir(subdir)\n",
    "                  if re.match(r'^(c1|c2).*.nii$', tpm)])\n",
    "    # collect relevant fMRI parameter files group analysis files\n",
    "    par_files = sorted([os.path.join(subdir, content)\n",
    "                       for subdir in FED_dirs\n",
    "                       for content in os.listdir(subdir)\n",
    "                       if re.match(r'.*(par|matrix)$', content)])\n",
    "\n",
    "    # append to files\n",
    "    files.extend(tpms)\n",
    "    files.extend(par_files)\n",
    "\n",
    "    # collect relevant fMRI parameter files group analysis files\n",
    "    tpm_group = sorted([os.path.join(subdir, content)\n",
    "                       for subdir in VBM_GROUP_dirs\n",
    "                       for content in os.listdir(subdir)\n",
    "                       if re.match(r'.*_6.nii$', content)])\n",
    "\n",
    "    fmri_group = sorted([os.path.join(subdir, content)\n",
    "                        for subdir in fMRI_GROUP_dirs\n",
    "                        for content in os.listdir(subdir)\n",
    "                        if re.match(r'.*(.nii).*', content)])\n",
    "\n",
    "\n",
    "    # collect general/group level files to data\n",
    "    content_group = tpm_group + fmri_group\n",
    "\n",
    "\n",
    "    # return the updated file lists for group and collective subjects\n",
    "    return files, content_group\n",
    "\n",
    "def update_files(files):\n",
    "    '''Update files in data using the collect_files() function or any other touple of the form: [allfilespersubject, groupfiles]'''\n",
    "    collsubs = files[0]\n",
    "    group = files[1]\n",
    "    # update group files\n",
    "    data[\"general\"] = group\n",
    "    # update subject files\n",
    "    for sub in FEDs:\n",
    "        # extend content\n",
    "        content_sub = [file for file in collsubs if re.match(fr'.*{sub}.*', file)]\n",
    "        # update files in data\n",
    "        data[sub][\"files\"] = content_sub\n",
    "    # remove all unnecessary variables\n",
    "    del collsubs, group, files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data():\n",
    "    '''Build the entire structure for data, general (for all) and per subject in FED'''\n",
    "\n",
    "    # collect image files\n",
    "    files = sorted([os.path.join(subdir, content)\n",
    "                   for subdir in FED_dirs\n",
    "                   for content in os.listdir(subdir)\n",
    "                   if re.match(r'.*.(nii|nii.gz|json)$', content)])\n",
    "    # for tissue probability and WM boundaries too (These are the originals! Processed files will be saved in FED_dir^^)\n",
    "    tpms = sorted([os.path.join(subdir, tpm)\n",
    "                   for subdir in TPM_dirs\n",
    "                   for tpm in os.listdir(subdir)\n",
    "                   if re.match(r'^(c1|c2).*.nii$', tpm)])\n",
    "    # collect relevant fMRI parameter files group analysis files\n",
    "    par_files = sorted([os.path.join(subdir, content)\n",
    "                       for subdir in FED_dirs\n",
    "                       for content in os.listdir(subdir)\n",
    "                       if re.match(r'.*(par|matrix)$', content)])\n",
    "\n",
    "    # extend files by the following\n",
    "    files.extend(tpms)\n",
    "    files.extend(par_files)\n",
    "\n",
    "    # collect relevant general/group level files\n",
    "    tpm_group = sorted([os.path.join(subdir, content)\n",
    "                       for subdir in VBM_GROUP_dirs\n",
    "                       for content in os.listdir(subdir)\n",
    "                       if re.match(r'.*_6.nii$', content)])\n",
    "\n",
    "    fmri_group = sorted([os.path.join(subdir, content)\n",
    "                        for subdir in fMRI_GROUP_dirs\n",
    "                        for content in os.listdir(subdir)\n",
    "                        if re.match(r'.*(.nii).*', content)])\n",
    "\n",
    "\n",
    "    # create a dictionary storing all files/subject and files concerning the entire analysis\n",
    "    data = {}\n",
    "\n",
    "    # collect general/group level files to data\n",
    "    data[\"general\"] = fmri_group + tpm_group\n",
    "\n",
    "    # assign data from files via subject IDs\n",
    "    for fed in FED_dirs:\n",
    "        ID = fed.rsplit('/', 1)[1]\n",
    "        # define partition for each subject\n",
    "        subject = {\"files\": [], \"parameters\": {}}\n",
    "        content = []\n",
    "        for file in files:\n",
    "            if re.match(fr'(.*{ID}.*)', file):\n",
    "                content.append(file)\n",
    "        subject[\"files\"].extend(content)\n",
    "        data[ID] = subject\n",
    "\n",
    "\n",
    "    # create shortcut to FEDs\n",
    "    FEDs = sorted([key for key in data.keys() if key != \"general\"])\n",
    "\n",
    "\n",
    "    # finally, return the resulting structure and FED list\n",
    "    return data, FEDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the initial data structure that holds all relevant information (\"I\" - prefix to all steps related)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base diectory for analysis toolboxes\n",
    "fsldir = \"/usr/share/fsl/5.0/\"\n",
    "spmdir = \"/opt/spm12-r7219/spm12_mcr/spm12/\"\n",
    "fsldatadir = f\"{fsldir}data/\"  # data directory in bash's $FSLDIR\n",
    "fsletcdir = f\"{fsldir}etc/\"  # this FSL directory contains other useful stuff. like alternative schedules etc.\n",
    "tpmdir = f\"{spmdir}tpm/\"  # data directory for SPM's TPM material\n",
    "\n",
    "# for relevant data files\n",
    "scriptdatadir = \"/home/martin/FED/\"\n",
    "credentials = f\"{scriptdatadir}FED_Subject_Covariates.xls\"\n",
    "modelinfo = f\"{scriptdatadir}FED_Day_2_modelparams.xls\"\n",
    "\n",
    "# for the basic data directories\n",
    "basedir = \"/fMRI/\"\n",
    "vbmdir = \"/VBM/\"\n",
    "\n",
    "# define fMRI group-level directories\n",
    "fMRI_GROUP_dirs = sorted([os.path.join(basedir, GROUP)\n",
    "                          for GROUP in os.listdir(basedir)\n",
    "                          if os.path.isdir(os.path.join(basedir, GROUP))\n",
    "                          and re.match(r'.*fMRI/(?!(FED|Nipype2FSL)).*', os.path.join(basedir, GROUP))])\n",
    "# define VBM group-level directories\n",
    "VBM_GROUP_dirs = sorted([os.path.join(vbmdir, GROUP)\n",
    "                          for GROUP in os.listdir(vbmdir)\n",
    "                          if os.path.isdir(os.path.join(vbmdir, GROUP))\n",
    "                          and re.match(r'.*VBM/.*template.*', os.path.join(vbmdir, GROUP))])\n",
    "# define functional subject directories\n",
    "FED_dirs = sorted([os.path.join(basedir, FED)\n",
    "                  for FED in os.listdir(basedir)\n",
    "                  if os.path.isdir(os.path.join(basedir, FED))\n",
    "                  and re.match(r'.*fMRI/(?!(MNI|Nipype2FSL|Grouplevel)).*', os.path.join(basedir, FED))])\n",
    "# define VBM subject directories\n",
    "TPM_dirs = sorted([os.path.join(vbmdir, FED)\n",
    "                  for FED in os.listdir(vbmdir)\n",
    "                  if os.path.isdir(os.path.join(vbmdir, FED))\n",
    "                  and re.match(r'.*VBM/(?!(DARTEL|VBM).*)', os.path.join(vbmdir, FED))])\n",
    "TPM_dirs = sorted([os.path.join(subdir, content)\n",
    "                   for subdir in TPM_dirs\n",
    "                   for content in os.listdir(subdir)\n",
    "                   if os.path.isdir(os.path.join(subdir, content))\n",
    "                   and re.match(r'.*newsegment.*', content)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build the initial data structure\n",
    "data = build_data()[0]\n",
    "\n",
    "# create a shortcut list for all FED subjects\n",
    "FEDs = build_data()[1]\n",
    "\n",
    "# safe an image of the current namespace to clean up at the end of this script\n",
    "data_varspace_init = dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. Get relevant file parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject(s) missing!\n",
      "First line of list-alignment:   FED006 FED006 FED007\n",
      "Control relevant files!\n",
      "\n",
      "\n",
      " The following subjects where excluded from further analysis due to false file numbers or missing data:  \n",
      " EPI:  ['FED006'] T1:  ['FED006']\n"
     ]
    }
   ],
   "source": [
    "# define exclusion set for FEDs that do not meet the requirements\n",
    "#GRE_excluded = []\n",
    "EPI_excluded = []\n",
    "T1_excluded = []\n",
    "\n",
    "# FED control\n",
    "for subject in FEDs:\n",
    "    # collect relevant files/FED for parameter extraction\n",
    "    #GREs = [file for file in data[subject][\"files\"]\n",
    "            #if re.match(r'(.*(_e1|_e2(?!_ph)).*.json)', file)]\n",
    "    EPIs = [file for file in data[subject][\"files\"]\n",
    "            if re.match(r'(.*(FMRI).*.json)', file)]\n",
    "    T1s = [file for file in data[subject][\"files\"]\n",
    "           if re.match(r'(.*(T1_MPRAGE).*.json)', file)]\n",
    "\n",
    "    # control number of files\n",
    "    if len(EPIs) != 1:\n",
    "        print(subject, \"\\n\", f\"Not exactly one FMRI sequence to read out ({len(EPIs)}).... investigate\", \"\\n\")\n",
    "        EPI_excluded.append(subject)\n",
    "\n",
    "    if len(T1s) != 1:\n",
    "        print(subject, \"\\n\", f\"Not exactly one T1 sequence to read out ({len(T1s)}).... investigate\", \"\\n\")\n",
    "        T1_excluded.append(subject)\n",
    "\n",
    "    # There seem to be several omissions in the GRER_FIELD data -> investigate\n",
    "#    elif len(GREs) < 2:\n",
    "#        print(subject, \"\\n\", f\"Not enough GRE_FIELD sequences to read out ({len(GREs)}).... exclude/investigate\", \"\\n\")\n",
    "#        GRE_excluded.append(subject)\n",
    "\n",
    "    # if there is more than one pair of magnitude images (e1+e2),\n",
    "    # take the first one as they are likely to be closer to the fMRI acquisition\n",
    "#    elif len(GREs) > 2:\n",
    "#        print(subject, \"\\n\", \"More than 2 GRE_FIELD sequences\",\n",
    "#              \"(\"+str(len(GREs))+\")\", \".... select only first two for TE extraction\", \"\\n\")\n",
    "\n",
    "\n",
    "# control presence of other relevant file content (presence of condition onset times, covariates of interest etc.)\n",
    "# read relevant content\n",
    "ID_cre = pd.read_excel(credentials, sheet_name=\"analysis\", usecols=['Sub Num FED_XXX'])\n",
    "ID_mod = pd.read_excel(modelinfo, sheet_name=\"analysis\", usecols=['Sub Num FED_XXX'])\n",
    "# bring content to list\n",
    "ID_cre = ID_cre['Sub Num FED_XXX'].tolist()\n",
    "ID_mod = ID_mod['Sub Num FED_XXX'].tolist()\n",
    "# model values are not unique (onset timings -> multiple entries^^) -> FIX for subject ID control \n",
    "ID_mod = list(set(ID_mod))\n",
    "# now for the control\n",
    "for subject, cre, mod in zip_longest(FEDs, ID_cre, ID_mod):\n",
    "    if subject[-1] != cre != mod:\n",
    "        print(\"subject(s) missing!\")\n",
    "        print(\"First line of list-alignment:  \",subject,f\"FED00{cre}\",f\"FED00{mod}\")\n",
    "        print(\"Control relevant files!\")\n",
    "        # control, but assume that the smaller value has to be eliminated,\n",
    "        # because, apparently, there are no values for it in (at least) one data file\n",
    "        FEDexcl = set(sorted([int(subject[-1]), cre, mod])[:-1])\n",
    "        FEDexcl = [f\"FED00{i}\" for i in FEDexcl]\n",
    "        # exclude respective subjects' functionals and structurals\n",
    "        EPI_excluded.extend(FEDexcl)\n",
    "        T1_excluded.extend(FEDexcl)\n",
    "        break\n",
    "\n",
    "# exclude FEDs based on file criteria\n",
    "print(\"\\n\\n\", \"The following subjects where excluded from further analysis due to false file numbers or missing data: \",\\\n",
    "      \"\\n\", \"EPI: \", sorted(EPI_excluded), \"T1: \", sorted(T1_excluded))\n",
    "\n",
    "# update data and FEDs based on prior exclusion\n",
    "[data.pop(sub) for sub in FEDs if sub in T1_excluded and sub in EPI_excluded]\n",
    "FEDs = [sub for sub in FEDs if sub not in T1_excluded and sub not in EPI_excluded]\n",
    "# re-define FED_dirs accordingly\n",
    "FED_dirs = sorted([os.path.join(basedir, f\"{fed}/\") for fed in FEDs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new subject list after initial data and image file checkup:\n",
      "\n",
      "['FED007', 'FED008', 'FED009', 'FED010', 'FED011', 'FED012', 'FED013', 'FED014', 'FED015', 'FED016', 'FED017', 'FED018', 'FED019', 'FED020', 'FED021', 'FED022', 'FED023', 'FED024', 'FED025', 'FED026', 'FED027', 'FED028', 'FED029', 'FED030', 'FED031', 'FED032', 'FED033', 'FED034', 'FED035', 'FED036', 'FED037', 'FED038', 'FED039', 'FED040', 'FED041', 'FED042', 'FED043', 'FED044', 'FED045', 'FED046', 'FED047', 'FED048', 'FED049', 'FED050', 'FED051', 'FED052', 'FED053', 'FED054', 'FED055', 'FED056', 'FED057', 'FED058', 'FED059', 'FED060', 'FED061', 'FED062', 'FED063', 'FED064', 'FED065', 'FED066', 'FED067', 'FED068']\n",
      "\n",
      "\n",
      "That leaves a total of  62 remaining subjects.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# control new subject list\n",
    "print(\"The new subject list after initial data and image file checkup:\\n\")\n",
    "print(FEDs)\n",
    "print(\"\\n\")\n",
    "print(\"That leaves a total of \", len(FEDs), \"remaining subjects.\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following parameters where not available from subjects' json files: \n",
      " ['FED063_EPI-DwellTime', 'FED064_EPI-DwellTime', 'FED065_EPI-DwellTime', 'FED066_EPI-DwellTime']\n",
      "The following parameters where available multiple times from subjects' json files: \n",
      " []\n"
     ]
    }
   ],
   "source": [
    "# Read json info data for all relevant parameters\n",
    "# define info of interest\n",
    "#GREspecs = [\"EchoTime\", \"PhaseEncodingDirection\"]\n",
    "EPIspecs = [\"EchoTime\", \"RepetitionTime\", \"EchoTrainLength\",\n",
    "            \"PhaseEncodingSteps\", \"PhaseEncodingDirection\",\n",
    "            \"DwellTime\", \"TotalReadoutTime\", \"EffectiveEchoSpacing\", \"PixelBandwidth\"]\n",
    "T1specs = [\"EchoTime\", \"RepetitionTime\",\n",
    "           \"PhaseEncodingSteps\", \"InversionTime\", \"PixelBandwidth\"]\n",
    "# create list to record missing parameters\n",
    "missing_params = []\n",
    "# create list to record multiple parameters\n",
    "multiple_params = []\n",
    "\n",
    "# Now for the parameter extraction\n",
    "for subject in FEDs:\n",
    "    # collect relevant files/FED for parameter extraction\n",
    "    #GREs=[file for file in data[subject][\"files\"]\n",
    "           #if re.match(r'(.*(_e1|_e2(?!_ph)).*.json)', file)]\n",
    "    T1s = [file for file in data[subject][\"files\"]\n",
    "           if re.match(r'(.*(T1_MPRAGE).*.json)', file)]\n",
    "    EPIs = [file for file in data[subject][\"files\"]\n",
    "            if re.match(r'(.*(FMRI).*.json)', file)]\n",
    "\n",
    "    # if there is more than one pair of magnitude images (e1+e2),\n",
    "    # take the first one as they are likely to be closer to the fMRI acquisition\n",
    "#    if len(GREs) > 2:\n",
    "#        print(subject, \"\\n\", \"More than 2 GRE_FIELD sequences\",\n",
    "#              \"(\"+str(len(GREs))+\")\", \".... selecting first two for TE extraction\", \"\\n\")\n",
    "#        GREs = GREs[0:2]\n",
    "\n",
    "    # T1 PARAMETERS\n",
    "    # collect parameters from the respective scan's json file\n",
    "    for file in T1s:\n",
    "        with open(file) as json_file:\n",
    "            info = json.load(json_file)\n",
    "            for param in T1specs:\n",
    "                # put params into data; use capital letters of parameter name as indicator in key\n",
    "                key = f\"T1_{''.join([char for char in param if char.isupper()])}\"\n",
    "                # if key does not exist -> create list with parameter\n",
    "                if key not in data[subject][\"parameters\"].keys():\n",
    "                    try:\n",
    "                        data[subject][\"parameters\"][key] = info[param]\n",
    "                    # if parameter does not exist in json file -> missing_params\n",
    "                    # append to GRE_excluded\n",
    "                    except KeyError:\n",
    "                        #print(f\"{subject}'s json file does not specify {param}\", \"\\n\",\n",
    "                             #\"noting issue ... \")\n",
    "                        missing_params.append(f\"{subject}_T1-{param}\")\n",
    "                        T1_excluded.append(subject)\n",
    "                        pass\n",
    "                # if key does exist -> append parameter to the list\n",
    "                if key in data[subject][\"parameters\"].keys():\n",
    "                    # if second value and first value for the parameter don't match,\n",
    "                    # there are either multiple .json files or multiple entries of the same parameter\n",
    "                    if info[param] != data[subject][\"parameters\"][key]:\n",
    "                        # print an info message\n",
    "                        #print(f\"{subject}'s json file does specify multiple entries under {key}:\",\"\\n\",\n",
    "                              #f'{data[subject][\"parameters\"][key]} and {info[param]}',\"\\n\",\n",
    "                             #\"inspect files!!!\")                            \n",
    "                        multiple_params.append(f\"{subject}_T1-{param}\")\n",
    "                    elif info[param] == data[subject][\"parameters\"][key]:\n",
    "                        pass\n",
    "\n",
    "    # EPI PARAMETERS\n",
    "    # collect parameters from the respective scan's json file\n",
    "    for file in EPIs:\n",
    "        with open(file) as json_file:\n",
    "            info = json.load(json_file)\n",
    "            for param in EPIspecs:\n",
    "                # put params into data; use capital letters of parameter name as indicator in key\n",
    "                key = f\"EPI_{''.join([char for char in param if char.isupper()])}\"\n",
    "                # if key does not exist -> create list with parameter\n",
    "                if key not in data[subject][\"parameters\"].keys():\n",
    "                    try:\n",
    "                        data[subject][\"parameters\"][key] = info[param]\n",
    "                    # if parameter does not exist in json file -> missing_params\n",
    "                    # append to EPI_excluded\n",
    "                    except KeyError:\n",
    "                        #print(f\"{subject}'s json file does not specify {param}\", \"\\n\",\n",
    "                             #\"noting issue ... \")\n",
    "                        missing_params.append(f\"{subject}_EPI-{param}\")\n",
    "                        EPI_excluded.append(subject)\n",
    "                        pass\n",
    "                # if key does exist -> append parameter to the list\n",
    "                if key in data[subject][\"parameters\"].keys():\n",
    "                    # if second value and first value for the parameter don't match,\n",
    "                    # there are either multiple .json files or multiple entries of the same parameter\n",
    "                    if info[param] != data[subject][\"parameters\"][key]:\n",
    "                        # print an info message\n",
    "                        #print(f\"{subject}'s json file does specify multiple entries under {key}:\",\"\\n\",\n",
    "                              #f'{data[subject][\"parameters\"][key]} and {info[param]}',\"\\n\",\n",
    "                             #\"inspect files!!!\")                              \n",
    "                        multiple_params.append(f\"{subject}_EPI-{param}\")\n",
    "                    elif info[param] == data[subject][\"parameters\"][key]:\n",
    "                        pass\n",
    "\n",
    "# show missing parameters from json files:\n",
    "print(\"The following parameters where not available from subjects' json files:\",\n",
    "      \"\\n\",sorted(missing_params))\n",
    "# show duplicate parameters from json files:\n",
    "print(\"The following parameters where available multiple times from subjects' json files:\",\n",
    "      \"\\n\",sorted(multiple_params))\n",
    "\n",
    "# exclude FEDs based on file criteria\n",
    "#print(\"\\n\\n\", \"The following subjects where excluded from further analysis due to false file numbers or missing parameters: \",\\\n",
    "#      \"\\n\", \"EPI: \", sorted(EPI_excluded), \"\\n\\n\",\n",
    "#     len(EPI_excluded), \"  subjects in total\")\n",
    "\n",
    "# update data and FEDs based on prior exclusion\n",
    "#[data.pop(sub) for sub in FEDs if sub in T1_excluded and sub in EPI_excluded]\n",
    "#FEDs=[sub for sub in FEDs if sub not in T1_excluded and sub not in EPI_excluded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " edited missing parameters ['FED063_EPI-DwellTime', 'FED064_EPI-DwellTime', 'FED065_EPI-DwellTime', 'FED066_EPI-DwellTime'] and inferred them from the other subjects. \n",
      " Parameter list is now full-rank. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Edit the read-in parameters into analysis format\n",
    "for sub in FEDs:\n",
    "    # calculate necessary parameters that are not in header information\n",
    "    # deltaTE for GRE_FIELD echo-based comparison\n",
    "    #data[subject][\"parameters\"][\"DeltaTE\"]= reduce(operator.sub, data[subject][\"parameters\"][\"GRE_ET\"])*-1*1000\n",
    "\n",
    "    # transfer phase encoding directions from field axes to voxel axes\n",
    "    # control field axes values\n",
    "    #print(sub)\n",
    "    #print(data[sub][\"parameters\"][\"EPI_PED\"])\n",
    "    epi_phasecodedir = data[sub][\"parameters\"][\"EPI_PED\"]\n",
    "    for char in epi_phasecodedir:\n",
    "        if char == \"i\":\n",
    "            data[sub][\"parameters\"][\"EPI_PED\"] = f\"x{epi_phasecodedir[1:]}\"\n",
    "        elif char == \"j\":\n",
    "            data[sub][\"parameters\"][\"EPI_PED\"] = f\"y{epi_phasecodedir[1:]}\"\n",
    "        elif char == \"k\":\n",
    "            data[sub][\"parameters\"][\"EPI_PED\"] = f\"z{epi_phasecodedir[1:]}\"\n",
    "\n",
    "    # same for T1s\n",
    "#    T1_phasecodedir = data[sub][\"parameters\"][\"T1_PED\"]\n",
    "#    for char in T1_phasecodedir:\n",
    "#        if char == \"i\":\n",
    "#            data[sub][\"parameters\"][\"T1_PED\"] = f\"x{T1_phasecodedir[1:]}\"\n",
    "#        elif char == \"j\":\n",
    "#            data[sub][\"parameters\"][\"T1_PED\"] = f\"y{T1_phasecodedir[1:]}\"\n",
    "#        elif char == \"k\":\n",
    "#            data[sub][\"parameters\"][\"T1_PED\"] = f\"z{T1_phasecodedir[1:]}\"\n",
    "\n",
    "\n",
    "# complete \"DwellTime\" entries for missing subjects\n",
    "# extract FEDs and params from missing_params\n",
    "mis_FED = [par.split('_', 1)[0] for par in missing_params]\n",
    "mis_Par = [par.split('-', 1)[1] for par in missing_params]\n",
    "\n",
    "# test the other FEDs' Dwelltimes and infer the missing value\n",
    "for par in set(mis_Par):\n",
    "    # get key to search\n",
    "    key = f\"EPI_{''.join([char for char in par if char.isupper()])}\"\n",
    "    # create list for parameter collection from all subjects' data\n",
    "    collection = []\n",
    "    for sub in FEDs:\n",
    "        # collect all FED IDs and parameters in one list\n",
    "        if key in data[sub][\"parameters\"]:\n",
    "            collection.append([sub, data[sub][\"parameters\"][key]])\n",
    "    # extract values for testing\n",
    "    par_vals = [x[1] for x in collection]\n",
    "    # if there is exactly one unique value, all values are identical\n",
    "    if len(set(par_vals)) == 1:\n",
    "        #print(f\"The values for {par} seem to be identical ... Substituting missing parameters accordingly\")\n",
    "        # set that value for all empty FEDs\n",
    "        rep_val = [val for val in set(par_vals)][0]\n",
    "        for empty in mis_FED:\n",
    "            data[empty][\"parameters\"][key] = rep_val\n",
    "    # if there is more than one unique value, check stuff\n",
    "    elif len(set(par_vals)) > 1:\n",
    "        #print(f\"The values for {par} are not identical ({len(set(par_vals))} different values).\",\n",
    "              #\"\\nfMRI acquisition may have been changed significantly over the course of data acquistion.\")\n",
    "        # assume that the omission in the json files happened during sequence alterations -> assign missing FEDs to latest group and latest value\n",
    "        rep_val = [val for val in set(par_vals)][-1]\n",
    "        for empty in mis_FED:\n",
    "            data[empty][\"parameters\"][key] = rep_val\n",
    "\n",
    "# inform about the parameter completion\n",
    "print(\"\\n\", f\"edited missing parameters {sorted(missing_params)} and inferred them from the other subjects.\",\n",
    "      \"\\n\", \"Parameter list is now full-rank.\", \"\\n\")\n",
    "\n",
    "# transfer DwellTime to ms\n",
    "for sub in FEDs:\n",
    "    epi_dwelltime = data[sub][\"parameters\"][\"EPI_DT\"] * 1000\n",
    "    #dwelltime_decimal = f\"{epi_dwelltime:.8f}\"\n",
    "    data[sub][\"parameters\"][\"EPI_DT\"] = epi_dwelltime\n",
    "\n",
    "# transfer Echotime to ms\n",
    "for sub in FEDs:\n",
    "    epi_echotime = data[sub][\"parameters\"][\"EPI_ET\"] * 1000\n",
    "    #echotime_decimal = f\"{epi_echotime:.8f}\"\n",
    "    data[sub][\"parameters\"][\"EPI_ET\"] = epi_echotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. Get / create relevant covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create covariates (FSL -> \"EVs\")\n",
    "# get sex, age, depression, time since clinical episode, severity of clinical episodes, number of clinical episodes, - data from list in .xlsx file(s)\n",
    "\n",
    "# files are already defined ^^\n",
    "# read relevant content\n",
    "content_cre = pd.read_excel(credentials, sheet_name=\"analysis\",\n",
    "                            usecols = ['Sub Num FED_XXX', 'Gender', 'Age', 'BDI 22 Score'])\n",
    "content_mod = pd.read_excel(modelinfo, sheet_name=\"analysis\",\n",
    "                            usecols = ['Sub Num FED_XXX', 'Condition', 'RT', 'COTcorrect'])\n",
    "\n",
    "# sort panda dataframe according to file sequence in FEDs (account for excluded)\n",
    "# define sort-by list (get FED_ID and format to fit entries in dataframe)\n",
    "model_FED_ID = [i[-3:].lstrip(\"0\") for i in FEDs]\n",
    "# transform values to integers to get values in model_FED_ID\n",
    "model_FED_ID = [int(i) for i in model_FED_ID]\n",
    "# define a categorical variable to sort a column and corresponding lines after\n",
    "content_cre['model_FED_ID'] = pd.Categorical(content_cre['Sub Num FED_XXX'],\n",
    "                                             categories = model_FED_ID, ordered=True)\n",
    "content_mod['model_FED_ID'] = pd.Categorical(content_mod['Sub Num FED_XXX'],\n",
    "                                             categories = model_FED_ID, ordered=True)\n",
    "# sort dataframes by indicator variables\n",
    "content_cre.sort_values(['model_FED_ID'], inplace=True)\n",
    "content_mod.sort_values(['model_FED_ID', 'Condition', 'COTcorrect'], inplace=True)\n",
    "\n",
    "# split experimental data by subject and create separate dataframes for each\n",
    "for sub, datasub in zip(FEDs, model_FED_ID):\n",
    "    # add the dataframe as additional item into subject's parameter dict\n",
    "    data[sub][\"parameters\"][\"covariates\"] = content_cre[content_cre['model_FED_ID'] == datasub]\n",
    "    data[sub][\"parameters\"][\"modelparams\"] = content_mod[content_mod['model_FED_ID'] == datasub]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create Bunch objects out of modelparams\n",
    "for sub in FEDs:\n",
    "    modelinfo = data[sub][\"parameters\"][\"modelparams\"]\n",
    "    # break conditions down to single occurences\n",
    "    conditions = modelinfo[\"Condition\"].unique().astype(str).tolist()\n",
    "    # make one list for each condition and rescale to secs\n",
    "    onsets = [modelinfo[modelinfo[\"Condition\"] == int(i)][\"COTcorrect\"].div(1000).tolist() for i in conditions]\n",
    "    # fill with duration to length of onsets and its elements\n",
    "    durations = [[1.5 for timing in range(0, len(trial))] for trial in onsets]  # what timeframe do you want to look at?\n",
    "    # build Bunch\n",
    "    trial_info = [Bunch(conditions = conditions,\n",
    "                        onsets = onsets,\n",
    "                        durations = durations)]\n",
    "    # add modelparameters in data with the newly created model_info Bunch\n",
    "    data[sub][\"parameters\"][\"trialinfo\"] = trial_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the changes to this data structure evoked by various stages within the pre-processing pipeline (\"II\" - prefix to all steps related)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II. Post-ART: Detect and remove subject outliers based on motion artefacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print length of outlier file for each subject to determine number of timepoints deemed outliers\n",
    "# by the current rapidart routine\n",
    "# define list of outlier numbers\n",
    "art_detect = {}\n",
    "outlier_nums = []\n",
    "\n",
    "for sub, subdir in zip(FEDs, FED_dirs):\n",
    "    artdetect_dir = subdir + \"artdetect/\"\n",
    "    if os.path.exists(artdetect_dir):\n",
    "        for file in os.listdir(artdetect_dir):\n",
    "            if re.match(r'.*outliers.txt', file):\n",
    "                outlier_file = os.path.join(artdetect_dir, file)\n",
    "                with open(outlier_file) as outliers:\n",
    "                    # amount of outliers is equal to the length of the outlier file\n",
    "                    amount = len(outliers.readlines())\n",
    "                    # for later comparison\n",
    "                    art_detect[sub] = amount\n",
    "                    # for present analysis\n",
    "                    outlier_nums.append(amount)\n",
    "                    #print(f\"For {subdir}, {amount} \",\n",
    "                          #\"outlier timepoints have been identified at the current rapidart-settings.\")\n",
    "    else:\n",
    "        print(f\"There is no directory named {artdetect_dir} yet\",\n",
    "              \"Check whether motion correction and outlier detection via rapidart have already been performed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "FED048, with 44 motion-related outliers,  exceeds an outlier threshold of 2z in a normal distribution of the outlier numbers. \n",
      "FED048 will therefore be excluded from further analysis\n"
     ]
    }
   ],
   "source": [
    "# write loop that executes only if the required files exist for each subject\n",
    "if len(art_detect.keys()) == len(FEDs) and len(outlier_nums) == len(FEDs):\n",
    "    # create z-scores from outlier numbers and determine outlying subjects to be removed from further analysis\n",
    "    # transform list into np.array\n",
    "    outlier_zs = np.abs(sp.stats.zscore(outlier_nums))\n",
    "\n",
    "    # print values exceeding a custom threshold\n",
    "    out_threshold = 2  # standard definition of z-based outliers\n",
    "    excess_motion = outlier_zs[outlier_zs > out_threshold]\n",
    "\n",
    "    # form a combined list (nums & zs have the same orientation and size) and identify the outlier(s)\n",
    "    outlier_combined = [[a, b] for a, b in zip(outlier_nums, outlier_zs)]\n",
    "    to_be_removed = [x[0] for x in outlier_combined if x[1] in excess_motion]\n",
    "\n",
    "    # compare art_detect with to_be_removed and identify the FED subject ID\n",
    "    for sub in FEDs:\n",
    "        if art_detect[sub] in to_be_removed:\n",
    "            print(\"\\n\")\n",
    "            print(f\"{sub}, with {art_detect[sub]} motion-related outliers, \",\n",
    "                  f\"exceeds an outlier threshold of {out_threshold}z in a normal distribution of the outlier numbers.\",\n",
    "                  f\"\\n{sub} will therefore be excluded from further analysis\")\n",
    "            # remove subject from data\n",
    "            data.pop(sub)\n",
    "\n",
    "# update FEDs\n",
    "FEDs = [sub for sub in FEDs if sub in data.keys()]\n",
    "# re-define FED_dirs accordingly\n",
    "FED_dirs = sorted([os.path.join(basedir, f\"{fed}/\") for fed in FEDs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The new subject list after motion correction and additional parameter calculations:\n",
      "\n",
      "['FED007', 'FED008', 'FED009', 'FED010', 'FED011', 'FED012', 'FED013', 'FED014', 'FED015', 'FED016', 'FED017', 'FED018', 'FED019', 'FED020', 'FED021', 'FED022', 'FED023', 'FED024', 'FED025', 'FED026', 'FED027', 'FED028', 'FED029', 'FED030', 'FED031', 'FED032', 'FED033', 'FED034', 'FED035', 'FED036', 'FED037', 'FED038', 'FED039', 'FED040', 'FED041', 'FED042', 'FED043', 'FED044', 'FED045', 'FED046', 'FED047', 'FED049', 'FED050', 'FED051', 'FED052', 'FED053', 'FED054', 'FED055', 'FED056', 'FED057', 'FED058', 'FED059', 'FED060', 'FED061', 'FED062', 'FED063', 'FED064', 'FED065', 'FED066', 'FED067', 'FED068']\n",
      "\n",
      "\n",
      "That leaves a total of  61 remaining subjects.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# control new subject list after motion artefact correction\n",
    "print(\"\\n\")\n",
    "print(\"The new subject list after motion correction and additional parameter calculations:\\n\")\n",
    "print(FEDs)\n",
    "print(\"\\n\")\n",
    "print(\"That leaves a total of \", len(FEDs), \"remaining subjects.\")\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:neuro] *",
   "language": "python",
   "name": "conda-env-neuro-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
